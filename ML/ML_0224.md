# Machine Learning

## Tensorflow

> - open source library
>
> - numerical computation
>
> - data flow graph
>
>   - 방향성이 있는 그래프
>
> - node
>
>   - 데이터의 입출력
>   - 연산
>
> - edge
>
>   - node와  node사이의 연결을 나타냄
>   - edge를 통해서 데이터가 전달됨
>     - Tensor(다차원 배열)
>
> - ```python
>   import tensorflow as tf
>   
>   node1 = tf.constant(10, dtype=tf.float32)     # 상수노드생성
>   node2 = tf.constant(20, dtype=tf.float32) 
>   
>   node3 = node1+node2
>   
>   # 그래프를 실행시키기 위해서는 Session이라는게 필요(runner)
>   sess = tf.Session() #2.x 버전에서는 Session이 삭제되었음
>   
>   print(sess.run(node3))
>   
>   ###############출력###################
>   
>   30.0
>   ```
>
> - ```python
>   import tensorflow as tf
>   
>   # Data Flow Graph에 입력값을 주려면 placeholder 사용
>   
>   node1 = tf.placeholder(dtype=tf.float32)
>   node2 = tf.placeholder(dtype=tf.float32)
>   
>   node3 = node1 + node2
>   
>   sess = tf.Session()
>   
>   result =sess.run(node3, feed_dict = {node1 :10,
>                                        node2:20})
>   print(result)
>   
>   ###############출력###################
>   
>   30.0
>   ```



## # Tensorflow를 이용해서 Simple Linear Regression 구현

> ```python
> # Tensorflow를 이용해서 Simple Linear Regression 구현
> 
> import numpy as np
> import pandas as pd
> 
> 
> # 1. training data set
> 
> x_data = (np.array([1,2,3,4,5])).reshape(5,1)   # python의 리스트로 training data 표현
> t_data = (np.array([3,5,7,9,11])).reshape(5,1) # lable
> 
> # 2. placeholder
> 
> X = tf.placeholder(shape=[None,1],dtype=tf.float32) 
> T = tf.placeholder(shape=[None,1],dtype=tf.float32)  
>                                                     
> 
> #  차원이 2차원 이상일때는 placeholder에 shape=[]명시해줘야함, shape=[None,1] 칼람의 갯수만 맞춰주고 나머진 상관 하지않음
> #  ex) X에 훈련 데이터로 (np.array([1,2,3,4,5])).reshape(5,1) 이게 X로들어오면 shape = 5,1이지만     
> #  predict 할때도 입력값이 X에 들어가기 때문에  값의 shape는 그때 그때 다르기 때문에 shape=[None,1] 독립변수의 갯수만신경 쓰겠다는뜻   
>     
> # 3. Weight & bias (y = Wx + b) => y = X dot W + b
> 
> W = tf.Variable(tf.random.normal([1,1]), name = 'weight') # W의 초기값을 랜덤으로 설정
> b = tf.Variable(tf.random.normal([1]), name = 'bias')   # b의 초기값을 랜덤으로 설정
> 
> # 4. Hypothesis or predict model
> H = tf.matmul(X,W)+b  # y = Wx+b  =>  2차원 행렬로 처리 => y = X dot W + b
>  
> # 5. W,b를 구하기 위해 평균제곱오차를 이용한 최소 제곱법을 통해
> #   loss function을 정의
> loss = tf.reduce_mean(tf.square(H - T))
> 
> # 6. train(학습용노드 , gradientdiscent )
> 
> train = tf.train.GradientDescentOptimizer(learning_rate = 1e-3).minimize(loss)
> #tf.train.GradientDescentOptimizer
> # 7. Session & 초기화
> 
> sess = tf.Session()
> sess.run(tf.global_variables_initializer())
> 
> # 8. 학습을 진행 
> # 반복학습을 진행(1 epoch : training data set 전체를 이용하여 1번 학습하는것)
> 
> for step in range(30000):
>     _, W_val, b_val, loss_val = sess.run([train, W, b, loss] , feed_dict={X:x_data,T:t_data}) # _ : 사용하지 않는 값을 의미
>     
>     if step % 3000 == 0:
>         print('W : {}, b : {}, loss : {}'.format(W_val,b_val,loss_val))
> 
> # 9. 학습이 종료된후 최적의 W와 b가 계산되고 이를 이용해 model이 완성
> # prediction()
> 
> result = sess.run(H, feed_dict={X : [[9]]})
> print("예측값 : {}".format(result))
> 
> ###############출력###################
> 
> W : [[2.3222616]], b : [-0.4851895], loss : 0.47644859552383423
> W : [[2.1468616]], b : [0.46978402], loss : 0.051170431077480316
> W : [[2.053253]], b : [0.8077398], loss : 0.006728122942149639
> W : [[2.0193143]], b : [0.93027276], loss : 0.0008849558653309941
> W : [[2.0070133]], b : [0.9746934], loss : 0.00011657839786494151
> W : [[2.0025456]], b : [0.990805], loss : 1.538696415082086e-05
> W : [[2.0009284]], b : [0.99665576], loss : 2.0363372641440947e-06
> W : [[2.0003362]], b : [0.9987872], loss : 2.675520249795227e-07
> W : [[2.0001333]], b : [0.99953127], loss : 4.0348151486568895e-08
> W : [[2.0000658]], b : [0.99977785], loss : 9.275026968680322e-09
> 예측값 : [[19.000296]]
> ```



## Simple Linear Regression을 python으로 구현

> ```python
> import numpy as np
> 
> # 1. training data set
> 
> x_data = (np.array([1,2,3,4,5])).reshape(5,1)   
> t_data = (np.array([3,5,7,9,11])).reshape(5,1) 
> 
> # 2. Weight & bias
> 
> W= np.random.rand(1,1)
> b= np.random.rand(1)
> 
> # 3. Hypothesis 모델학습이 끝나고 예측 값을 구할때 사용
> 
> def predict(x):
>     y = x.dot(W) + b  # y = Wx+b
>     return y
> 
> # 4. loss_function
> 
> def loss_func(input_obj):
>     # input_obj : [W,b]
>     input_W = input_obj[0]
>     input_b = input_obj[1]
>     
>     y = np.dot(x_data, input_W) + input_b
>     
>     return np.mean(np.power((t_data-y),2))
> 
> def numerical_derivative(f,x):
>     
>     # f : 미분하려고 하는 다변수 함수
>     # x : 모든 값을 포함하는 numpy array  ex) f'(1.0, 2.0) = (8.0, 15.0)
>     # [W,b]
>     delta_x = 1e-4
>     derivative_x = np.zeros_like(x)    # [0 0]
>     
>     it = np.nditer(x, flags=['multi_index'])
>     
>     while not it.finished:
>         
>         idx = it.multi_index   # 현재의 iterator의 index를 추출 => tuple형태로 나와요
>         #print('현재의 idx : {}'.format(idx))        
>         
>         tmp = x[idx]     # 현재 index의 값을 잠시 보존.
>                          # delta_x를 이용한 값으로 ndarray를 수정한 후 편미분을 계산
>                          # 함수값을 계산한 후 원상복구를 해 줘야 다음 독립변수에
>                          # 대한 편미분을 정상적으로 수행할 수 있어요!
>         #print('현재 temp : {}'.format(tmp))   
>         x[idx] = tmp + delta_x        
>         fx_plus_delta = f(x)    # f([1.00001, 2.0])   => f(x + delta_x)
>         
> 
>         x[idx] = tmp - delta_x
>         fx_minus_delta = f(x)    # f([0.99999, 2.0])   => f(x - delta_x)
>         
>         derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
>         
>         x[idx] = tmp
>         
>         it.iternext()
>         
>     return derivative_x
> 
> # learning_rate 설정
> 
> learning_rate =1e-4
> 
> for step in range(300000):
>     input_param = np.concatenate((W.ravel(),b.ravel()),axis=0) # W.ravel : 2차원 매트릭스를 1차원 벡터로 만듬
>     derivative_result = learning_rate * numerical_derivative(loss_func,input_param)
>     
>     W = W - derivative_result[:1].reshape(1,1) # W 갱신
>     b = b - derivative_result[1:]              # b 갱신
>     if step % 30000 == 0:
>         print('W : {}, b : {}'.format(W,b))
>         
> ###############출력###################
> 
> W : [[0.4172542]], b : [0.11344513]
> W : [[2.04180971]], b : [0.84905368]
> W : [[2.01516246]], b : [0.9452587]
> W : [[2.00549873]], b : [0.98014784]
> W : [[2.00199414]], b : [0.99280053]
> W : [[2.00072318]], b : [0.99738908]
> W : [[2.00026226]], b : [0.99905314]
> W : [[2.00009511]], b : [0.99965662]
> W : [[2.00003449]], b : [0.99987547]
> W : [[2.00001251]], b : [0.99995484]
> ```



## sickit-learn을 이용하여 구현

> ```python
> import numpy as np
> 
> from sklearn import linear_model
> 
> # 1. training data set
> 
> x_data = (np.array([1,2,3,4,5])).reshape(5,1)   
> t_data = (np.array([3,5,7,9,11])).reshape(5,1) 
> 
> # 2. linear regression model을 생성
> model = linear_model.LinearRegression()
> 
> # 3. 학습진행
> model.fit(x_data, t_data)
> 
> # 4. Weight & bias 출력
> print('W : {}, b : {}'.format(model.coef_, model.intercept_))
> 
> # 5. predict
> print(model.predict([[9]]))
> 
> ###############출력###################
> 
> W : [[2.]], b : [1.]
> [[19.]]
> ```



